---
title: 04-IK分词器
url: https://www.yuque.com/gaollard/efekv4/zpv14b
---

前面的范例创建索引、搜索数据时都是使用默认的分词器，因为存储的都是中文，所以分词效果不太理想，会把 text 的字段分成`一个个汉字`，为了更好地对中文内容进行分词，需要更加智能的 IK 分词器。

```shell
#对内容"内心没有分别心，就是真正的苦行"利用IK分词器进行分析
POST _analyze
{
  "analyzer": "ik_max_word",
  "text": "内心没有分别心，就是真正的苦行"
}

{
  "tokens" : [
    {
      "token" : "内心",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "CN_WORD",
      "position" : 0
    },
    {
      "token" : "没有",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "分别",
      "start_offset" : 4,
      "end_offset" : 6,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "心",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "CN_CHAR",
      "position" : 3
    },
    {
      "token" : "就是",
      "start_offset" : 8,
      "end_offset" : 10,
      "type" : "CN_WORD",
      "position" : 4
    },
    {
      "token" : "真正",
      "start_offset" : 10,
      "end_offset" : 12,
      "type" : "CN_WORD",
      "position" : 5
    },
    {
      "token" : "的",
      "start_offset" : 12,
      "end_offset" : 13,
      "type" : "CN_CHAR",
      "position" : 6
    },
    {
      "token" : "苦行",
      "start_offset" : 13,
      "end_offset" : 15,
      "type" : "CN_WORD",
      "position" : 7
    }
  ]
}
```

IK 分词器的两种分词模式:

1）ik\_max\_word：对文本进行最细粒度的拆分。

2）ik\_smart：对文本进行最粗粒度的拆分。



### ik\_max\_word

- 使用 ik\_max\_word 模式将"中华人民共和国国歌"拆分为 `"[中华人民共和国，中华人民，中华，华人，人民共和国，人民，共和国，共和，国，国歌]"`，产生了各种可能的组合，即不同的词。
- 使用 ik\_max\_word 模式将"我是中国人"拆分为 `"[我，是，中国人，中国，国人]"`，产生了各种可能的组合



### ik\_smart

- 使用 ik\_smart 模式将"中华人民共和国国歌"拆分为"\[中华人民共和国，国歌]"，产生了尽可能少的组合。
- 使用 ik\_smart 模式将"我是中国人"拆分为 `"[我，是，中国人]"`，产生尽可能少的组合



### 创建使用 IK 分词器的索引映射

一般在创建索引时会明确指定分词的模式，总共有两种操作：一种是让所有 text 类型的字段都使用分词模式，另一种是给每一种 text 类型的字段指定分词模式。



#### 让所有 text 类型的字段都使用分词模式

```shell
#创建索引模板，所有"text"类型的字段都使用IK分词器的"ik_max_word"模式
PUT myindex_ik
{
  "settings":{
    "analysis":{
      "analyzer":{
        "ik":{
          "tokenizer":"ik_max_word"
        }
      }
    }
  },
  "mappings":{
      "properties":{
        "field1":{
          "type":"text"
        },
        "field2":{
          "type":"integer"
        },
        "field3":{
          "type":"text"
        },
        "field4":{
          "type":"text"
        }
    }
  }
}
```



#### 给每一种 text 类型的字段指定分词模式

```shell
PUT myindex_ik_01
{
  "mappings":{
      "properties":{
        "field1":{
          "type":"text",
          "analyzer": "ik_max_word",
                     "search_analyzer": "ik_max_word"
        },
        "field2":{
          "type":"integer"
        },
        "field3":{
          "type":"text",
          "analyzer": "standard",
             "search_analyzer": "standard"
        },
        "field4":{
          "type":"text",
          "analyzer": "ik_max_word",
            "search_analyzer": " ik_smart "
        }
    }
  }
}
```



### 存储和搜索使用不同的分词模式

```shell
#创建索引映射，存储时使用IK分词器的ik_max_word模式，搜索时使用IK分词器的ik_smart模式
PUT /clayindex_ik
{
  "mappings": {
    "properties": {
    "name":{
      "type": "keyword"
    },
    "address":{
      "type": "text",
      "analyzer": "ik_max_word",
      "search_analyzer": "ik_smart" #设置默认的搜索分词模式
    },
    "age":{
      "type": "integer"
    }
  }
}

#插入文档数据
POST  /clayindex_ik/_doc
{
  "name":"曹操",
  "address":"魏国",
  "age":18
}

#插入文档数据
POST  /clayindex_ik/_doc
{
  "name":"贾诩",
  "address":"魏国",
  "age":19
}

#插入文档数据
POST  /clayindex_ik/_doc
{
  "name":"诸葛亮",
  "address":"蜀国",
  "age":37
}

#插入文档数据
POST  /clayindex_ik/_doc
{
  "name":"关羽",
  "address":"蜀国",
  "age":35
}

#插入文档数据
POST  /clayindex_ik/_doc
{
  "name":"周瑜",
  "address":["吴国","蜀国"],
  "age":25
}

#全文搜索address等于"魏国"的数据，并指定分词模式
POST clayindex_ik/_doc/_search
{
  "query":{
     "match": {
       "address": {
         "query": "魏国",
          "analyzer": "ik_smart" #这句可以不写，因为默认就是这种模式
       }
     }
  }
}
```

需要注意的是，因为我们在设置映射模板时使用最细粒度进行分词存储（分词尽可能多），所以在搜索时可以指定对最细粒度的分词模式和最粗粒度的分词模式分别搜索。在正式项目的使用中也推荐这种做法，存储时选择尽量细的分词规则，这样在搜索时可以指定符合具体项目要求的分词模式。
