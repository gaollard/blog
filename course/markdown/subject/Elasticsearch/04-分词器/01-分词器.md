---
title: 01-分词器
url: https://www.yuque.com/gaollard/efekv4/ypgn4p
---

“分词” 是在 Elasticsearch 中进行存储和全文搜索的一个很重要的部分，因为只有选择合适的分词器才能更高效地进行全文搜索。Elasticsearch 中提供了多种内置的分词器，针对不同的场景可以使用不同的分词器。这些分词器只对 `text` 类型字段有效，而对于 `keyword` 类型字段无效。本章主要内容

- simple 分词器详解
- simple\_pattern 分词器详解
- simple\_pattern\_split 分词器详解
- text类型和keyword类型的区别
- IK分词器插件的各种环境安装
- IK分词器的使用模式和范例



### simple 分词器

simple 分词器是对字母文本进行分词拆分，并将分词后的内容转换成小写格式。范例如下：

```shell
#对指定内容根据"simple"分词器进行分词
POST _analyze
{
  "analyzer": "simple",
  "text": "Our usual study and experience are our most powerful support at a critical moment"
}
```

"Our usual study and experience are our most powerful support at acritical moment"被分成了14个关键字并且所有字母都是小写的。



### simple\_pattern 分词器

根据正则表达式进行分词的分词器:

```shell
#创建映射并定义字段内容分词的正则表达式
PUT myindex-simple_pattern
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "simple_pattern", #正则表达式表示，如果连续有3个数字在一起，则可以被当作一个单词
          "pattern": "[0123456789]{3}" 
        }
      }
    }
  }
}

#对指定内容根据"my_analyzer"分词器进行分词
POST myindex-simple_pattern/_analyze
{
  "analyzer": "my_analyzer",
  "text": "fd-123-4567-890-xxd9-689-x987"
}


{
  "tokens" : [
    {
      "token" : "123",
      "start_offset" : 3,
      "end_offset" : 6,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "456",
      "start_offset" : 7,
      "end_offset" : 10,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "890",
      "start_offset" : 12,
      "end_offset" : 15,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "689",
      "start_offset" : 21,
      "end_offset" : 24,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "987",
      "start_offset" : 26,
      "end_offset" : 29,
      "type" : "word",
      "position" : 4
    }
  ]
}
```



### simple\_pattern\_split 分词器

simple\_pattern\_split（指定分词符号）分词器比 `simple_pattern` 分词器功能更有限，但是分词效率较高。默认模式下，它的分词匹配符号是空字符串。需要注意的是，使用此分词器应该根据业务进行配置，而不是简单地使用默认匹配模式。范例如下：

```shell
#创建索引映射并指定字段内容分词匹配符号
PUT myindex-simple_pattern_split
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "simple_pattern_split",
          #当遇到"-"符号就进行分词
          "pattern": "-"
        }
      }
    }
  }
}

#对指定内容根据"-"分隔符匹配规则进行分词
POST myindex-simple_pattern_split/_analyze
{
  "analyzer": "my_analyzer",
  "text": "fd-123-4567896-890-xxd9-689-x987"
}

{
  "tokens" : [
    {
      "token" : "fd",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "123",
      "start_offset" : 3,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "4567896",
      "start_offset" : 7,
      "end_offset" : 14,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "890",
      "start_offset" : 15,
      "end_offset" : 18,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "xxd9",
      "start_offset" : 19,
      "end_offset" : 23,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "689",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "x987",
      "start_offset" : 28,
      "end_offset" : 32,
      "type" : "word",
      "position" : 6
    }
  ]
}
```



### standard 分词器

standard（标准）分词器是Elasticsearch中默认的分词器，它是基于Unicode文本分割算法进行分词的。范例如下：

```shell
#对指定内容根据standard分词器进行分词
POST _analyze
{
  "analyzer": "standard",
  "text": "Our usual study and experience are our most powerful support at a critical moment"
}

["Our","usual","study","and","experience","are","our","most","powerful ","support"," at"," a"," critical"," moment"]
```

standard分词器还提供了表4-1所示的两种参数。

![](https://s3.airtlab.com/elasticsearch/20220428213706.png)

```shell
#创建索引映射，对分词规则进行配置，规则是分词后单词的最大长度是6，根据英语语法进行分析
PUT standard_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "english_analyzer":{
          "type":"standard",
          "max_token_length":6,
          "stopwords":"_english_"
        }
      }
    }
  }
}

#对指定内容根据如上规则进行分词
POST standard_index/_analyze
{
  "analyzer": "english_analyzer",
  "text": "Our usual study and experience are our most powerful support at a critical moment"
}
```
